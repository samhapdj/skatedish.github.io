
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"> 
<html> 

<head>
<meta content="text/html; charset=iso-8859-1" http-equiv="Content-Type">
<title>Lab 10: ML project</title>
<link type="text/css" rel="stylesheet" href="projects.css"/>
</head>

<body> 
  
<h2>Lab 10: Machine learning project<br>Due May 8 by midnight</h2>

<blockquote>
    <center>
    <img src="machinelearninglogo.jpg" ><br>
    </center>
</blockquote>



<h3>Machine Learning Contests</h3>

In the past decade, a large amount of work in machine learning has been motivated by various contests and challenges.
One of the best known was the Netflix prize (<a href="http://www.netflixprize.com/">official site</a>, <a href="https://en.wikipedia.org/wiki/Netflix_Prize">Wikipedia</a>), which offered $1M to the team that could improve the site's recommendation system by 10%.
The Netflix prize was claimed in 2009; since then machine learning contests have become commonplace.

<p>
For this project you will take on a machine learning challenge of your choice from <a href="https://www.kaggle.com/competitions">kaggle</a>.
Some of these contests are currently active, with prizes available.
However, you are welcome to work on contest problems that have expired or (with approval) on another machine learning problem of your choosing.
On kaggle, you can see old under <a href="https://www.kaggle.com/competitions?sortBy=deadline&group=all&page=1&segment=allCategories">all competitions</a>.

<p>
Many of these challenges involve large data sets that could quickly blow through your disk quota.
To avoid this, you can save them to <tt>/scratch</tt> (<a href="https://www.cs.swarthmore.edu/help/scratchlocal.html">instructions</a>), which is unlimited, but isn't backed up.
Also, take a look at the department's <a href="https://www.cs.swarthmore.edu/help/long.html">suggestions for long running jobs</a>.

<p>
In order to download data, you will need to sign up for a free account.
Kaggle also has a <a href="https://www.kaggle.com/discussion">discussion forum</a>, which may have useful suggestions, especially if you are working on an active contest.

<h3>Tools</h3>

For this project you <b>should not</b> use the code that you have written in previous labs to implement machine learning algorithms.
Instead, you should make as much use as you can of the machine learning libraries <a href="http://scikit-learn.org/stable/">scikit-learn</a> and <a href="https://keras.io/">keras</a>.
We have seen both of these libraries before: in labs 7 and 8.
Keras is an excellent resource for neural networks; scikit-learn should be your go-to library for all other machine learning algorithms.

<h4>Scikit-Learn</h4>

<p>
<a href="http://scikit-learn.org/">Scikit-learn</a> is a collection of Python libraries that implement a large number of machine learning algorithms.
We previously used the sklearn implementation of support vector machines in lab 8.
Scikit-learn has a huge collection of classification, clustering, and regression techniques.
Here is the documentation for sklearn implementations of many of the algorithms we have studied:

<p>
<ul>
<li><a href="http://scikit-learn.org/stable/modules/svm.html#classification">Support vector machines</a>
<li><a href="http://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbor-algorithms">K-nearest neighbors</a>
<li><a href="http://scikit-learn.org/stable/modules/naive_bayes.html">Naive Bayes</a>
<li><a href="http://scikit-learn.org/stable/modules/tree.html">Decision trees</a>
<li><a href="http://scikit-learn.org/stable/modules/clustering.html#k-means">K-means</a>
<li><a href="http://scikit-learn.org/stable/modules/mixture.html#mixture">EM for Gaussian mixture models</a>
<li><a href="http://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering">Hierarchical clustering</a>
<li><a href="http://scikit-learn.org/stable/modules/decomposition.html#decompositions">Principal component analysis</a>
</ul>

<p>
Scikit-learn also has modules for <a href="http://scikit-learn.org/stable/modules/preprocessing.html">preprocessing data</a> and for evaluating models with <a href="http://scikit-learn.org/stable/model_selection.html">cross validation</a>.
Feel free to poke around in the scikit-learn documentation for other tools and algorithms.

<h4>Keras</h4>

<a href="https://keras.io/">Keras</a> provides an easy-to-use python interface to the TensorFlow and Theano deep learning libraries.
In lab 7, we used keras to train neural networks on the MNIST handwritten digit data set.
Scikit-learn has a rudimentary neural network implementation, but if you plan on using neural networks in you project, you should use Keras.




<h3>Ensemble Methods</h3>

The Netflix prize was open for nearly three years, during which time several research teams improved prediction scores using a variety of machine learning methods.
The prize was ultimately won by a meta-group of research teams whose meta-algorithm ran each team's classifier in parallel and made recommendations based on their joint results.
This outcome demonstrates the power of <b>ensemble learning</b>.

<p>
Ensemble learning is based on the idea that combining the output of many weak classifiers can make a strong classifier that outperforms all of its component parts.
Ensemble learning is feasible as long as all of the component classifiers are useful (they perform better than random guessing) and not entirely redundant (they sometimes give different answers).

<p>
Two common methods for ensemble learning are <a href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)">boosting</a> and <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating">bagging</a>.
Boosting works by training many weak classifiers (such as shallow decision trees) on the same data and then taking a plurality vote or weighted average over their outputs in order to classify a new point.
Bagging works by training many highly specific classifiers (such as deep decision trees) on random subsets of the data, and again taking a vote or average over their output labels.

<p>
You are expected to use at least one ensemble method in your project.
The ensemble method may or may not be the best learning algorithm for your task, but your writeup should report the results of testing the ensemble against its component algorithms.
Several variations on boosting and bagging are implemented by scikit-learn.
Documentation can be found in the <a href="http://scikit-learn.org/stable/modules/ensemble.html">ensemble methods</a> section.




<h3>Submitting</h3>

<p>
Before the deadline, you need to submit the following things through git:

<ul>
		<li>The python code you wrote to implement your expermints.</li>
		<li>A README file explaining how to run your code and access your data set(s).</li>
		<li>The latex file with your write-up: <tt>project.tex</tt></li>
</ul>

<p>
<b>In addition, you must turn in a hard copy of the writeup pdf outside my office.</b>

<p>In the LaTex file, <tt>project.tex</tt>, you will describe your project.
This file already contains a basic structure that you should follow.
Feel free to change the section headings, or to add additional sections.
Recall that you use <tt>pdflatex</tt> to convert the LaTex into a pdf file.
Here is a <a href="project.pdf">template</a> for your paper.

<p>
As your project develops and you create more files, be sure to use git to 
add, commit, and push them.  Run: <tt>git status</tt> to check that all of
the necessary files are being tracked in your git repo.
Don't forget to update the README so that I can test your code!

</body> 
</html>
